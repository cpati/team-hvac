{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stance Detection Factor\n",
    "#### This notebook is a subset of Spam_detection_and_Stance_detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset is used from fakenewschallenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stances2 = pd.read_csv('../train_stances.csv', sep=',')\n",
    "dataset_body2 = pd.read_csv('../train_bodies.csv', sep=',')\n",
    "UCI_Aggregator_load = pd.read_csv('../uci-news-aggregator.csv',sep=',')\n",
    "UCI_Aggregator = UCI_Aggregator_load.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fakeness impact based on the claim stance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* agree = 0 (If most of the claims agree with the news, the news is possibly not fake)\n",
    "* disagree = 1 (If most of the claims disagree with the news, the news is possibly fake)\n",
    "* discuss = 0.5 (If there is a lot of discussion happening around, the news may or may not be fake)\n",
    "* unrealted = 1 (If the stances are mostly unrealted, the news will most likely be fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a doc2vec model for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(dataset_body2['articleBody'])]\n",
    "#print (tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =0)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "model.save(\"doc2vec.model\")\n",
    "#print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the model is trained load it to use it on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "model= Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_body2['index'] = dataset_body2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This python fucntion will scrape the fakenewschallenge dataset to get claims for all the news articles present in new aggregator dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the relevant claims are found a new dataset will be returned which will have just the required details like news article and respective claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claims():\n",
    "    from gensim.models.doc2vec import Doc2Vec\n",
    "    model= Doc2Vec.load(\"doc2vec.model\")\n",
    "    dataset_body2['index'] = dataset_body2.index\n",
    "    count=1\n",
    "    uci_news=0\n",
    "    fnc_news_body_index=0\n",
    "    for i in range(14204, 400000):\n",
    "        claim1=UCI_Aggregator['TITLE'][i]\n",
    "        claim1.lower()\n",
    "        new_test_data = word_tokenize(claim1.lower())\n",
    "        v2 = model.infer_vector(new_test_data)\n",
    "        similar_documents = model.docvecs.most_similar([v2], topn = 1)\n",
    "        myarray = np.asarray(similar_documents)\n",
    "        new_a=myarray.squeeze()\n",
    "        similarity_score=new_a[1]\n",
    "        article_id = new_a[0]\n",
    "        if float(similarity_score) > 0.95:\n",
    "            #print (\"{}\".format(count))\n",
    "            #print (\"At index {}, Claim - {}\".format(i, claim1))\n",
    "            uci_news = i\n",
    "            #print(\"** Similar news articles **\")\n",
    "            #print(dataset_body2.loc[dataset_body2['index'] == int(new_a[0]), 'articleBody'])\n",
    "            fnc_news_body_index = int(new_a[0])\n",
    "            count+=1\n",
    "    UCI_Aggregator['TITLE'][uci_news]\n",
    "    dataset_body2['articleBody'][fnc_news_body_index]\n",
    "    Body_ID = dataset_body2.loc[dataset_body2['index'] == fnc_news_body_index, 'Body ID']\n",
    "    new_df = dataset_stances2[dataset_stances2['Body ID'] == int(Body_ID)]\n",
    "    new_df2 = new_df.join(new_df['Stance'].str.get_dummies())\n",
    "    return new_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will analyse the stances of all the claims from the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stance_check():\n",
    "    df = get_claims()\n",
    "    arr=df['Stance'].value_counts().index\n",
    "    for i in arr:\n",
    "        if i == 'discuss':\n",
    "            Stance_check=0.5\n",
    "        elif i == 'agree':\n",
    "            Stance_check=0\n",
    "        elif i == 'disagree':\n",
    "            Stance_check=1\n",
    "        else:\n",
    "            Stance_check=1\n",
    "    return Stance_check\n",
    "#stance_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanceDetection:\n",
    "    def __init__(self,news):\n",
    "        self.news=news\n",
    "    def predict(self):\n",
    "        return stance_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
