{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution of Chidananda Pati/012506949\n",
    "## It has code for 2 factors\n",
    "- Spell Checking - More spelling error means mostly writeen by un-professionals and may be a fake news\n",
    "- Political Affiliation - I initially thought of geography , but could not apply that to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_count</th>\n",
       "      <th>false_Count</th>\n",
       "      <th>half_true_count</th>\n",
       "      <th>mostly_true_count</th>\n",
       "      <th>pants_on_fire_count</th>\n",
       "      <th>venue_speach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        label                                          statement  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker           speaker_job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state party_affiliation  barely_true_count  false_Count  \\\n",
       "0     Texas        republican                0.0          1.0   \n",
       "1  Virginia          democrat                0.0          0.0   \n",
       "2  Illinois          democrat               70.0         71.0   \n",
       "3       NaN              none                7.0         19.0   \n",
       "4   Florida          democrat               15.0          9.0   \n",
       "\n",
       "   half_true_count  mostly_true_count  pants_on_fire_count  \\\n",
       "0              0.0                0.0                  0.0   \n",
       "1              1.0                1.0                  0.0   \n",
       "2            160.0              163.0                  9.0   \n",
       "3              3.0                5.0                 44.0   \n",
       "4             20.0               19.0                  2.0   \n",
       "\n",
       "          venue_speach  \n",
       "0             a mailer  \n",
       "1      a floor speech.  \n",
       "2               Denver  \n",
       "3       a news release  \n",
       "4  an interview on CNN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['id','label','statement','subject','speaker','speaker_job','state',\n",
    "        'party_affiliation','barely_true_count','false_Count',\n",
    "        'half_true_count','mostly_true_count','pants_on_fire_count','venue_speach'];\n",
    "df_lair=pd.read_csv('../train.tsv',sep='\\t',header=None,names=columns,index_col=False);\n",
    "df_lair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chidanandapati/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chidanandapati/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chidanandapati/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Proprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for text preprocessing\n",
    "- lowercase the text\n",
    "- word tokenization\n",
    "- remove stop words and non alphanumeric charaters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "def text_preprocessing(df_base,column):\n",
    "    df=df_base.copy()\n",
    "    # lowercase the text\n",
    "    df[column]=df[column].str.lower()\n",
    "    # word tokenization\n",
    "    df[column]=df[column].map(lambda x: nltk.word_tokenize(x))\n",
    "    # remove stop words and non alphanumeric charaters\n",
    "    df[column]=df[column].map(lambda x: [w for w in x if (not w in stop_words) and w.isalpha()])\n",
    "    # lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    df[column]=df[column].map(lambda x: [ wordnet_lemmatizer.lemmatize(w) for w in x])    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    df[column]=df[column].map(lambda x: [porter.stem(w) for w in x] )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling text_preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_count</th>\n",
       "      <th>false_Count</th>\n",
       "      <th>half_true_count</th>\n",
       "      <th>mostly_true_count</th>\n",
       "      <th>pants_on_fire_count</th>\n",
       "      <th>venue_speach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>[say, anni, list, polit, group, support, abort...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>[declin, coal, start, start, natur, ga, took, ...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>[hillari, clinton, agre, john, mccain, vote, g...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>[health, care, reform, legisl, like, mandat, f...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>[econom, turnaround, start, end, term]</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        label                                          statement  \\\n",
       "0   2635.json        false  [say, anni, list, polit, group, support, abort...   \n",
       "1  10540.json    half-true  [declin, coal, start, start, natur, ga, took, ...   \n",
       "2    324.json  mostly-true  [hillari, clinton, agre, john, mccain, vote, g...   \n",
       "3   1123.json        false  [health, care, reform, legisl, like, mandat, f...   \n",
       "4   9028.json    half-true             [econom, turnaround, start, end, term]   \n",
       "\n",
       "                              subject         speaker           speaker_job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state party_affiliation  barely_true_count  false_Count  \\\n",
       "0     Texas        republican                0.0          1.0   \n",
       "1  Virginia          democrat                0.0          0.0   \n",
       "2  Illinois          democrat               70.0         71.0   \n",
       "3       NaN              none                7.0         19.0   \n",
       "4   Florida          democrat               15.0          9.0   \n",
       "\n",
       "   half_true_count  mostly_true_count  pants_on_fire_count  \\\n",
       "0              0.0                0.0                  0.0   \n",
       "1              1.0                1.0                  0.0   \n",
       "2            160.0              163.0                  9.0   \n",
       "3              3.0                5.0                 44.0   \n",
       "4             20.0               19.0                  2.0   \n",
       "\n",
       "          venue_speach  \n",
       "0             a mailer  \n",
       "1      a floor speech.  \n",
       "2               Denver  \n",
       "3       a news release  \n",
       "4  an interview on CNN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=text_preprocessing(df_lair,'statement')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Cloud\n",
    "To understand most frequent words in the corpus\n",
    "- pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements=''.join(df_lair['statement'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower max_font_size, change the maximum number of word and lighten the background:\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(statements)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion is corpus looks like is mostly political and money related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec to create a vector representation of processed words for every article\n",
    "- pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_sentences(articles, label_type):\n",
    "    labelledSentences = []\n",
    "    for i,_d in enumerate(articles):\n",
    "        labelledSentences.append(LabeledSentence(_d, label_type[i]))\n",
    "    return labelledSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labelled=labelled_sentences(df_train['statement'],df_train['label'])\n",
    "df_train_labelled[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(df_train['statement'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data_words=[x.words for x in tagged_data]\n",
    "tagged_data_words_1D=[]\n",
    "for row in range(len(tagged_data_words)):\n",
    "    for col in range(len(tagged_data_words[row])):\n",
    "        tagged_data_words_1D.append(tagged_data_words[row][col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=300\n",
    "w2v = Word2Vec(size=n_dim, min_count=0)\n",
    "w2v.build_vocab(tagged_data_words)\n",
    "w2v.train(tagged_data_words,total_examples=w2v.corpus_count,epochs=w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar('polit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_statements=df_train[['statement','label']]\n",
    "df_train_statements_vectorized=df_train_statements['statement'].map(lambda x: [w2v[w] for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_statements['statement']=df_train_statements_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_train_statements['label']=le.fit_transform(df_train_statements['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_statements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(le.inverse_transform(df_train_statements['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    return np.mean(doc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx in range(len(df_train_statements['statement'])):\n",
    "    for col_idx in range(len(df_train_statements['statement'][row_idx])):\n",
    "        df_train_statements['statement'][row_idx][col_idx]=document_vector(df_train_statements['statement'][row_idx][col_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_statements['statement']=df_train_statements['statement'].map(document_vector)\n",
    "df_train_statements.drop(index=4497,inplace=True)\n",
    "df_train_statements.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making word vec to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for x in df_train_statements['statement'].values:\n",
    "    texts.append(x)   \n",
    "X=pd.DataFrame(texts)\n",
    "X=X.fillna(0)\n",
    "y=df_train_statements[['label']]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size = .2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "gnb_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Lets try with 100 trees\n",
    "num_trees=100\n",
    "rf=RandomForestClassifier(n_estimators=num_trees)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_statements_d2v=df_train[['statement','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"d2v.model.all\")\n",
    "print(\"Model Saved\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Doc2Vec.load(\"d2v.model.all\")\n",
    "texts=[]\n",
    "for x in df_train_statements_d2v['statement']:\n",
    "    texts.append(model.infer_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label(x):\n",
    "    if (x == 'true' or x=='mostly-true' or x=='half-true'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "replace_label('true')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(texts)\n",
    "#y=df_train_statements_d2v[['label']]\n",
    "y=df_train_statements_d2v['label'].map(lambda x:replace_label(x))\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size = .3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr_D2V = LogisticRegression(C=100)\n",
    "logisticRegr_D2V.fit(X_train, y_train)\n",
    "lr_pred = logisticRegr_D2V.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Lets try with 100 trees\n",
    "num_trees=100\n",
    "rf=RandomForestClassifier(n_estimators=num_trees)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means for vectorized docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a very long time\n",
    "#kclusterer = KMeansClusterer(5, distance=nltk.cluster.util.cosine_distance, repeats=5)\n",
    "#assigned_clusters = kclusterer.cluster(texts, assign_clusters=True)\n",
    "#assigned_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, word in enumerate(texts):  \n",
    "#    print (texts[i] , assigned_clusters[i])\n",
    "#print (texts[1] , assigned_clusters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km\n",
    "import matplotlib.pyplot as plt\n",
    "def kmeans_plot(k):\n",
    "    kmeans=km(n_clusters=k,random_state=0).fit(texts)\n",
    "    labels=kmeans.labels_\n",
    "    centroids=kmeans.cluster_centers_    \n",
    "    colors = ['r','g','b','y','c','m','r','g','b','y','c','m','r','g','b','y','c','m']\n",
    "    fig, ax=plt.subplots()\n",
    "    \n",
    "    for i in range(k):\n",
    "        points=np.array([texts[j] for j in range(len(texts)) if labels[j]==i])\n",
    "        ax.scatter(points[:,0],points[:,1],s=7,c=colors[i])\n",
    "    ax.scatter(centroids[:,0],centroids[:,1],marker='*',s=200,c='#050505')     \n",
    "kmeans_plot(5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Modelling to see if we can find any topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "texts=df_train['statement']\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=7, id2word = dictionary, passes=20)\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Seems like are all related to Politics, broadly followings:\n",
    "- Tax Cut\n",
    "- Health Care\n",
    "- Immigration and Tax Policies\n",
    "- 2016 Presidential Campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Document with higher tf-idf has higher political affilication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggel Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kgl=pd.read_csv('./kaggel/train.csv',header=0)\n",
    "df_kgl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kgl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"truth\",data=df_kgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#df_kgl[['subjects']].apply(lambda x:ast.literal_eval(x))\n",
    "df_kgl['subjects_list']=df_kgl['subjects'].apply(lambda x:ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects=[]\n",
    "for col in df_kgl['subjects_list']:\n",
    "    for val in col:\n",
    "        subjects.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "letter_counts = Counter(subjects).most_common(20)\n",
    "letter_counts\n",
    "#df = pd.DataFrame.from_dict(letter_counts, orient='index')\n",
    "#df.plot(kind='bar',figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kgl_hc=df_kgl[df_kgl['subjects'].str.contains('Health Care')]\n",
    "df_kgl_hc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kgl_hc_train=text_preprocessing(df_kgl_hc,'statement')\n",
    "df_kgl_hc_train['statement'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"truth\",data=df_kgl_hc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "texts=df_kgl_hc_train['statement']\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=7, id2word = dictionary, passes=20)\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Political Affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_political_affiliation=['president','george','bush','administration','republican','democrats','barack','obama','hillary','clinton','donald','trump','senate','house']\n",
    "#vocab_political_affiliation=['president','george','bush','administration']\n",
    "#vocab_political_affiliation=['president','administration','republican','democrats','barack','clinton','donald','trump','senate','house']\n",
    "len(vocab_political_affiliation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_statments=df_lair['statement'].str.lower().values.tolist()\n",
    "l_statments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tokenized_statements=df_train['statement'].map(lambda x:' '.join(x))\n",
    "type(l_tokenized_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab_political_affiliation)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_weights(tokenized_statements):\n",
    "    tfidf_matrix = vectorizer.fit_transform(tokenized_statements)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tf_idf_vals=[]\n",
    "    for i in range(len(tfidf_matrix.toarray())):\n",
    "        tf_idf_doc=np.zeros(len(vocab_political_affiliation))\n",
    "        feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[i,x] for x in feature_index])\n",
    "        count=0\n",
    "        for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "            if (w in vocab_political_affiliation):\n",
    "                tf_idf_doc[count]=s\n",
    "                count+=1\n",
    "        tf_idf_vals.append(tf_idf_doc.tolist())   \n",
    "                #tf_idf_vals=tf_idf_vals[~(tf_idf_vals==0).all(1)]\n",
    "                #tf_idf_vals[np.all(tf_idf_vals == 0,axis=1)]\n",
    "    return np.array(tf_idf_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label(x):\n",
    "    if (x == 'true' or x=='mostly-true' or x=='half-true'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "replace_label('true')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf_idf_weights(l_tokenized_statements)\n",
    "y=df_lair['label'].map(lambda x:replace_label(x))\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size = .4, random_state = 1)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(C=100)\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "lr_pred = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statment(statement):\n",
    "    statement=nltk.word_tokenize(statement.lower())\n",
    "    statement=[w for w in statement if (not w in stop_words) and w.isalpha()]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    statement=[ wordnet_lemmatizer.lemmatize(w) for w in statement]\n",
    "    porter = PorterStemmer()\n",
    "    statement=[porter.stem(w) for w in statement]\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_statment(\"Says the Annies List political group supports \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_statments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news=\"says the annies list political group supports third-trimester abortions on demand\";\n",
    "#statments=df_train['statement'].map(lambda x:' '.join(x))\n",
    "#statments=statments.append(pd.Series(' '.join(tokenize_statment(test_news))))\n",
    "#statments=statments.reset_index()\n",
    "print(statments.iloc[-1])\n",
    "#statments\n",
    "tf_idf_weights(statments)[1]\n",
    "#political_affiliation_checker(test_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec for political affilication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_political_affiliation=['president','george','bush','administration','republican','democrats','barack','obama','hillary','clinton','donald','trump','senate','house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_doc_pa = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(vocab_political_affiliation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_doc_pa)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_doc_pa,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"d2v.model.pa\")\n",
    "print(\"Model Saved\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_statements_d2v=df_train[['statement','label']]\n",
    "model= Doc2Vec.load(\"d2v.model.pa\")\n",
    "texts=[]\n",
    "for x in df_train_statements_d2v['statement']:\n",
    "    texts.append(model.infer_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_label(x):\n",
    "    if (x == 'true' or x=='mostly-true' or x=='half-true'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "replace_label('true')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(texts)\n",
    "#y=df_train_statements_d2v[['label']]\n",
    "y=df_train_statements_d2v['label'].map(lambda x:replace_label(x))\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size = .3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr_D2V_PA = LogisticRegression(C=100)\n",
    "logisticRegr_D2V_PA.fit(X_train, y_train)\n",
    "lr_pred_pa = logisticRegr_D2V_PA.predict(X_test)\n",
    "import pickle\n",
    "s = pickle.dumps(logisticRegr_D2V_PA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.19      0.28      1331\n",
      "          1       0.58      0.86      0.69      1741\n",
      "\n",
      "avg / total       0.55      0.57      0.51      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test,lr_pred_pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def political_affiliation_checker(news):\n",
    "    data_pred=[]\n",
    "    data_pred.append(model.infer_vector(news))\n",
    "    lrg_pa = pickle.loads(s)\n",
    "    pred_conf=lrg_pa.predict_proba(data_pred)\n",
    "    #print(pred_conf)\n",
    "    return pred_conf[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliticalAffilicationDetector:\n",
    "    def __init__(self,news):\n",
    "        self.news=news\n",
    "    def predict(self):\n",
    "        return political_affiliation_checker(self.news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46289932276995244"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_affiliation_checker(\"Says the Annies List political group supports third-trimester abortions on demand.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr_D2V_PA.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
